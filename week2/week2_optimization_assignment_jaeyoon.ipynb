{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tobig's 14기 2주차 Optimization 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent 구현하기\n",
    "    1) 빈 칸을 채워 함수 완성하기\n",
    "    2) 강의 내용과 코드에 대해 공부한 내용 적기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    우선 기본적인 설정과 데이터를 불러서 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Label       200 non-null    int64  \n",
      " 1   bias        200 non-null    int64  \n",
      " 2   experience  200 non-null    float64\n",
      " 3   salary      200 non-null    int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 6.4 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기\n",
    "    아래의 과정을 통해 데이터셋을 train/test로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,1:],data.iloc[:,0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    위의 train_test_split 함수를 사용하면 train과 test로 데이터셋을 나눌 수 있다. 순서대로 X데이터와 y데이터를 분할 해주고, test로 사용할 데이터 비율을 설정한다. 마지막으로 random_state를 설정하여 무작위 추출법을 선택해준다. 데이터 수를 아래와 같이 확인해보면 비율에 맞춰 나눠진 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "    experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춘다. StandardScaler는 평균을 0, 분산을 1로 맞춰주는 scaler이다. X_train에 fit과 transform을 시킨다. 하지만 X_test에는 scaler를 transform만 시킨다. 이미 X_train에 fit했기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01342451, 0.75749943, 0.81042703])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p = 1/(1+e^{-\\beta*X})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X,parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z += X[i]*parameters[i]\n",
    "    p = 1/(1 + np.exp(-z))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7205356089874139"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Function\n",
    "\n",
    "Object Function: 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수이다. 로지스틱 회귀의 목적함수는 Cross entropy loss 함수로 이를 최소화 하는 것이 목적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    cross_entropy_i 함수는 하나의 행에 대해서만 구해진 결과이다. 전체 데이터셋에 대한 loss를 구하기 위해서는 cross_entropy를 사용해야한다. cross_entropy에서도 logistic 함수를 사용해서 p를 대체해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X,parameters)\n",
    "    loss = (y*np.log(p)+(1-y)*np.log(1-p))\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(X_set, y_set, parameters) :\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += cross_entropy_i(X,y,parameters)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136.46912287791267"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(X_train, y_train, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of Cross Entropy\n",
    "\n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)= -\\sum(y_{i}-p_{i})x_{ij}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_ij_cross_entropy(X, y, parameters, j):\n",
    "    p = logistic(X,parameters)\n",
    "    gradient = (y-p)*X[j]\n",
    "    return -gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1284628001501548"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij_cross_entropy(X_train.iloc[0, :], y_train.iloc[0], parameters, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "Batch Gradient Descent: 한 번의 학습으로 모든 데이터셋에 대한 기울기를 구하는 방법이다. 확실히 모수를 정확하게 추측할 수 있지만, 전체 데이터를 다뤄야 하기 때문에 시간이 오래걸린다는 단점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients_bgd(X_train, y_train, parameters):\n",
    "    gradients = [0 for i in range(len(parameters))]\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        X = X_train.iloc[i, :]\n",
    "        y = y_train.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij_cross_entropy(X,y,parameters,j)\n",
    "            \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32.42931465880591, 20.793944764729, 52.032738948244905]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients_bgd = get_gradients_bgd(X_train,y_train,parameters)\n",
    "gradients_bgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Stochastic Gradient Descent: 한 번의 학습에 임의의 데이터를 선택하여 이에 대한 기울기만을 구한다. 장점으로는 시간이 확실히 짧게 걸리지만 정확성이 많이 떨어진다는 단점이 있다. 임의 추출한 데이터가 이상치거나 좋지 않다면 큰 오류에 빠질 가능성이 크다.\n",
    "\n",
    "위의 batch gradients descent는 전체를 다루기 때문에 전체 열을 다 지정해준다. 하지만 아래의 Stochastic Gradient Descent는 임의의 데이터를 뽑아야 하기 때문에 random.random()를 통해 0부터 1사이의 난수를 설정한다. 그리고 이를 전체 데이터 갯수에 더하여 임의의 정수를 설정하고 이를 각 데이터셋에 인덱싱하여 gradient를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients_sgd(X_train, y, parameters):\n",
    "    gradients = [0 for i in range(len(parameters))]\n",
    "    r = int(random.random()*len(X_train))\n",
    "    X = X_train.iloc[r,:]\n",
    "    y = y_train.iloc[r]\n",
    "    \n",
    "    for j in range(len(parameters)):\n",
    "        gradients[j] = get_gradient_ij_cross_entropy(X,y,parameters,j)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28045589646563346, -0.36700464355056905, 0.012332885764594723]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients_sgd = get_gradients_sgd(X_train, y_train, parameters)\n",
    "gradients_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "\n",
    "gradient를 구한 뒤, 이를 parameters에 반영하여 새로운 parameters를 구하는 함수를 짜준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate) :\n",
    "    for i in range(len(parameters)):\n",
    "        gradients[i] *= learning_rate\n",
    "    parameters -= gradients\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.31086864,  0.54955998,  0.29009964])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_parameters(parameters, gradients_bgd, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "learning_rate = 학습률로 다음 실행을 할 때, 이동하는 거리의 크기를 의미한다. 보폭으로 생각하면 쉽게 이해할 수 있다.  \n",
    "max_iter = 아래 함수를 최대 반복하는 횟수를 정한 것이다.  \n",
    "tolerance = 허용 오차에 대한 값으로 loss의 차이가 이 값보다 작아진다면 더 이상의 학습은 무의미하다고 판단해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate=0.01, max_iter=100000, tolerance=0.0001, optimizer=\"bgd\"):\n",
    "    count = 1\n",
    "    point = 100 if optimizer == \"bgd\" else 10000\n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.array([random.random() for i in range(N)])\n",
    "    gradients = [0 for i in range(N)]\n",
    "    loss = cross_entropy(X_train,y_train,parameters)\n",
    "    \n",
    "    while count < max_iter :\n",
    "        \n",
    "        if optimizer == \"bgd\" :\n",
    "            gradients = get_gradients_bgd(X_train,y_train,parameters)\n",
    "        elif optimizer == \"sgd\" :\n",
    "            gradients = get_gradients_sgd(X_train,y_train,parameters)\n",
    "        \n",
    "        if count%point == 0 :\n",
    "            new_loss = cross_entropy(X_train,y_train,parameters)\n",
    "            print(count, \"loss: \", new_loss, \"params: \", parameters, \"gradients :\", gradients)\n",
    "            \n",
    "            if abs(new_loss-loss) < tolerance*len(y_train) :\n",
    "                break\n",
    "            loss = new_loss\n",
    "        \n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        count += 1\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loss:  45.37226786927311 params:  [-1.62586594  3.48089692 -3.30677472] gradients : [0.27434673418587807, -0.9025996746890962, 0.8435794398989779]\n",
      "200 loss:  44.79947350748002 params:  [-1.78253636  3.99294022 -3.78281256] gradients : [0.0827635638639195, -0.2689333526120898, 0.24870473971619977]\n",
      "300 loss:  44.74012132128334 params:  [-1.83428801  4.16084338 -3.93783321] gradients : [0.029813191495298985, -0.09657911494329788, 0.08902179690820157]\n",
      "400 loss:  44.73210817180596 params:  [-1.85344016  4.22285387 -3.99495774] gradients : [0.011334491169068095, -0.036679252831533275, 0.033768960604028904]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.85344016,  4.22285387, -3.99495774])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train)\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning\n",
    "\n",
    "Hyper Parameter들을 다르게 해서 학습하면 다른 값이 나온다는 것을 확인할 수 있다. 위와 아래의 차이는 단순히 bgd 방법이냐 sgd방법이냐 뿐이다. 나온 parameter 결과값은 매우 유사하나 근소하게 차이를 보이고 있다는 것을 알 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loss:  46.748563585552866 params:  [-1.61139766  3.04237471 -2.80497528] gradients : [0.05629369733911125, -0.08168909985130467, -0.06436255193696788]\n",
      "20000 loss:  45.17711796228033 params:  [-1.67072425  3.60457323 -3.51753641] gradients : [0.0001691675927710734, -0.0001912348703587342, 0.00014134189310041124]\n",
      "30000 loss:  44.94748673485817 params:  [-1.87107234  3.89042091 -3.76034086] gradients : [0.18752079532641897, -0.15184854995991595, -0.17729154705754796]\n",
      "40000 loss:  44.85613488539769 params:  [-1.88228526  4.15010281 -3.82592442] gradients : [-0.214793737906392, -0.055664790377436754, 0.1180678792072771]\n",
      "50000 loss:  44.81915804414755 params:  [-1.74787969  4.24123692 -4.01551577] gradients : [0.020615278977041976, 0.031785938367840744, 0.0444207032471967]\n",
      "60000 loss:  44.84540498265975 params:  [-1.80753045  4.18144957 -4.07592739] gradients : [0.11608581211380833, -0.060912871259809154, -0.05615287057627504]\n",
      "70000 loss:  44.735659082082904 params:  [-1.89795132  4.29336593 -4.06848334] gradients : [-0.5490199645671863, -0.1031570306248332, 0.12071417628972342]\n",
      "80000 loss:  45.00069194267007 params:  [-2.05839554  4.31734554 -4.01424031] gradients : [0.6009709762426361, -0.7007784562272643, -1.1231624147602468]\n",
      "90000 loss:  44.73641842671419 params:  [-1.89495089  4.33013065 -4.07912205] gradients : [0.01585784398152314, -0.021316605415349944, -0.013946790262679087]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.97545177,  4.31444203, -4.20846053])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train,y_train,learning_rate=0.01,max_iter=100000, tolerance=0.0001, optimizer=\"sgd\")\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 new_param_bgd들은 optimizer를 bgd 방식으로 tolerance나 learning_rate를 다르게 해서 구한 값이다. 우선 아래의 2개는 learning_rate를 0.001과 0.1로 두고 진행한 결과들이다.\n",
    "\n",
    "learning_rate가 0.001일 때 눈에 띄는 점은 처음 설정했던 학습률보다 학습률이 작아짐에 따라 연산을 훨씬 더 많이해 parameter를 구하는 시간이 오래 걸린다는 점이다. 반면 learning_rate가 0.1일 때 눈에 띄는 점은 이전보다 연산 시간이 확실이 줄었다는 점이다. \n",
    "\n",
    "그러나 new_param_bgd1과 new_param_bgd2는 전혀 다른 parameter값을 도출했다. new_param_bgd1의 경우 위에서 구한 parameter와 유사한 값을 도출했다. 게다가 해당 parameter는 위의 parameter보다 좀 더 정교할 것으로 조심스럽게 추측된다. 반면 new_param_bgd2에서 구한 parameter의 경우 위에서 구한 parameter와 차이가 매우 크다. learning_rate가 커짐으로써, 정교한 도출이 불가능 했을 것을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loss:  64.01506183004118 params:  [-0.9317551   1.06854516 -0.98800202] gradients : [3.206987567394756, -8.342824050825612, 8.375584350326815]\n",
      "200 loss:  54.82206246225797 params:  [-1.12640563  1.7318601  -1.63963722] gradients : [1.3653671070369882, -5.256164526208162, 5.107696841056792]\n",
      "300 loss:  50.8927160672398 params:  [-1.24330045  2.17255708 -2.0658685 ] gradients : [1.011652084164729, -3.7000626981681597, 3.5611546419061475]\n",
      "400 loss:  48.81368818379668 params:  [-1.33327042  2.49430335 -2.37443265] gradients : [0.7986009850878115, -2.7972274100972405, 2.672813280623333]\n",
      "500 loss:  47.57667056642643 params:  [-1.40520807  2.74309343 -2.61149576] gradients : [0.6465794734279636, -2.2103975374329417, 2.100222465486679]\n",
      "600 loss:  46.78383613418733 params:  [-1.46401996  2.94266152 -2.80069385] gradients : [0.533850012458394, -1.79910692795566, 1.7017462136560275]\n",
      "700 loss:  46.249091653096166 params:  [-1.5129618   3.10682679 -2.95569222] gradients : [0.4477976805176331, -1.4954013244980227, 1.409280535610524]\n",
      "800 loss:  45.87482842035243 params:  [-1.55427558  3.244354   -3.08510321] gradients : [0.38041612378957324, -1.2624831056286738, 1.186150047273963]\n",
      "900 loss:  45.60546567976239 params:  [-1.58955269  3.36116053 -3.19470787] gradients : [0.32650018720659135, -1.0786811430662777, 1.0108658365369845]\n",
      "1000 loss:  45.40734094378882 params:  [-1.6199563   3.461435   -3.28857684] gradients : [0.2825718249290005, -0.9303870583413854, 0.8699989385891583]\n",
      "1100 loss:  45.259066814561606 params:  [-1.64635958  3.54825401 -3.36968574] gradients : [0.24623788088848486, -0.8086133783694613, 0.7547208610541714]\n",
      "1200 loss:  45.14652696997171 params:  [-1.66943346  3.62394533 -3.44027606] gradients : [0.21580476885717706, -0.7071781713102959, 0.6589852706133897]\n",
      "1300 loss:  45.06011127533445 params:  [-1.68970401  3.69031324 -3.50207832] gradients : [0.19004288676825704, -0.621684637615874, 0.5785093674335527]\n",
      "1400 loss:  44.993107989969104 params:  [-1.70759095  3.74878472 -3.55645614] gradients : [0.16803885110970235, -0.5489168542220921, 0.510172670907395]\n",
      "1500 loss:  44.940728347948884 params:  [-1.72343426  3.80050752 -3.60450271] gradients : [0.14910027852361235, -0.48646606125853353, 0.45164638364626764]\n",
      "1600 loss:  44.89949328533702 params:  [-1.73751293  3.84641823 -3.64710737] gradients : [0.13269280503870776, -0.4324908721746419, 0.4011563214361917]\n",
      "1700 loss:  44.866835944268416 params:  [-1.75005849  3.88729059 -3.68500283] gradients : [0.11839740211236663, -0.38555848361362066, 0.35732640195292054]\n",
      "1800 loss:  44.84083718990882 params:  [-1.76126504  3.92377068 -3.71879933] gradients : [0.10588079552092684, -0.3445366070276446, 0.3190724412701998]\n",
      "1900 loss:  44.820045574991845 params:  [-1.7712967   3.95640303 -3.74901003] gradients : [0.09487454379607015, -0.3085181364852641, 0.28552833264670063]\n",
      "2000 loss:  44.803352319330656 params:  [-1.78029332  3.98565032 -3.77607005] gradients : [0.08515996514023266, -0.2767675128175094, 0.2559936311842358]\n",
      "2100 loss:  44.78990295379778 params:  [-1.78837488  4.01190851 -3.80035103] gradients : [0.07655709374378483, -0.24868180908561383, 0.22989562591319476]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.78837488,  4.01190851, -3.80035103])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd1 = gradient_descent(X_train,y_train,learning_rate=0.001,max_iter=100000, tolerance=0.0001, optimizer=\"bgd\")\n",
    "new_param_bgd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loss:  54.49240770871232 params:  [-2.21396901  4.49424959 -5.59684438] gradients : [1.221938005260047, -11.823787992571697, -13.060305783230227]\n",
      "200 loss:  54.52384907027382 params:  [-2.21446701  4.49340886 -5.59853543] gradients : [1.2242236359004415, -11.844699616062492, -13.078572538779918]\n",
      "300 loss:  54.52388542842257 params:  [-2.21446677  4.4934055  -5.59853515] gradients : [1.2242281047293968, -11.844730678459449, -13.078597203595692]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.21446677,  4.4934055 , -5.59853515])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd2 = gradient_descent(X_train,y_train,learning_rate=0.1,max_iter=100000, tolerance=0.0001, optimizer=\"bgd\")\n",
    "new_param_bgd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 2개는 tolerance 값을 다르게 해서 구한 값들이다. tolerance를 각각 0.001과 0.00001로 두고 학습을 진행했다. 결과는 아래와 같다.\n",
    "\n",
    "tolerance를 달리 했을 때, 가장 먼저 눈에 들어오는 것은 연산 횟수이다. tolerance가 0.001일 때는 연산 횟수가 줄었다. 반면 tolerance가 0.00001일 때는 연산 횟수가 늘어났다.\n",
    "\n",
    "또한 new_param_bgd3 - new_param_bgd - new_param_bgd4 순으로 첫 번째 parameter와 세 번째 parameter는 점점 작아지고, 두 번째 parameter는 점점 커짐을 알 수 있다. 이는 tolerance의 변화로 인해 발생하는 것으로 판단할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loss:  45.38421021680113 params:  [-1.62383166  3.47420253 -3.30051702] gradients : [0.27718750562929056, -0.9121344319121912, 0.8526127348348397]\n",
      "200 loss:  44.800520440058484 params:  [-1.78192444  3.99095173 -3.78097355] gradients : [0.08341729907921945, -0.27106803739690244, 0.25068875305077876]\n",
      "300 loss:  44.74025583724082 params:  [-1.83406775  4.16012985 -3.9371755 ] gradients : [0.03002909554773979, -0.09727972950593751, 0.08966882118423665]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.83406775,  4.16012985, -3.9371755 ])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd3 = gradient_descent(X_train,y_train,learning_rate=0.01,max_iter=100000, tolerance=0.001, optimizer=\"bgd\")\n",
    "new_param_bgd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loss:  45.38682629890534 params:  [-1.62338896  3.47274549 -3.2991549 ] gradients : [0.27780704210020407, -0.914214463172494, 0.8545836631741213]\n",
      "200 loss:  44.80074904162993 params:  [-1.78179147  3.99051962 -3.78057392] gradients : [0.08355944316448567, -0.27153221219716994, 0.25112018494922733]\n",
      "300 loss:  44.74028518464761 params:  [-1.83401991  4.15997486 -3.93703264] gradients : [0.030076002769462373, -0.09743194702066492, 0.0898093978313852]\n",
      "400 loss:  44.732131774199594 params:  [-1.85333826  4.22252411 -3.99465415] gradients : [0.011431283611858564, -0.036992683263876086, 0.03405773601107258]\n",
      "500 loss:  44.73093690689457 params:  [-1.86075236  4.24651209 -4.01673397] gradients : [0.004430467472657393, -0.014331718309012809, 0.013188645980435579]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.86075236,  4.24651209, -4.01673397])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd4 = gradient_descent(X_train,y_train,learning_rate=0.01,max_iter=100000, tolerance=0.00001, optimizer=\"bgd\")\n",
    "new_param_bgd4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 적합 및 모델 평가\n",
    "\n",
    "최종적으로 만든 gradient_descent함수에 train 데이터를 적용한다. 이후 test 데이터와 새롭게 얻은 parameter들로 y_predict 값을 구한다. 구한 y_predict 값과 y_test값을 바탕으로 모델을 평가한다. 평가방법은 회귀분석 시간에 사용했던 평가지표를 활용한다. 우선 y_predict 값을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loss:  45.35081024751799 params:  [-1.6295776   3.49310766 -3.31818662] gradients : [0.2691891195636057, -0.8853005136100661, 0.827195737384705]\n",
      "200 loss:  44.79757830884621 params:  [-1.78365673  3.99658073 -3.7861792 ] gradients : [0.0815683774464118, -0.2650310672098955, 0.2450782731790454]\n",
      "300 loss:  44.73987733967241 params:  [-1.83469165  4.16215097 -3.93903848] gradients : [0.029417730749418766, -0.0952958790086214, 0.0878367615295439]\n",
      "400 loss:  44.73207300410715 params:  [-1.85359363  4.2233505  -3.99541496] gradients : [0.011188744268236323, -0.03620730601412034, 0.03333414480542455]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.85359363,  4.2233505 , -3.99541496])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train)\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=[]\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:],new_param_bgd)\n",
    "    if p>0.5:\n",
    "        y_predict.append(1)\n",
    "    else:\n",
    "        y_predict.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38,  2],\n",
       "       [ 1,  9]], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import*\n",
    "tn,fp,fn,tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   위에서 만든 y_predict 값과 y_test 값을 바탕으로 confusion matrix를 만든다. 만들어진 matrix는 위와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp+tn)/(tp+fn+fp+tn)\n",
    "precision = (tp)/(tp+fp)\n",
    "recall = (tp)/(tp+fn)\n",
    "f1_score = 2*(precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.94\n",
      "precision:  0.8181818181818182\n",
      "recall:  0.9\n",
      "f1_score:  0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: \", accuracy)\n",
    "print(\"precision: \", precision)\n",
    "print(\"recall: \", recall)\n",
    "print(\"f1_score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만들어진 confusion matrix를 바탕으로 모델 평가에 사용될 accuracy, precision, recall, f1_score 값은 위와 같다. accuracy는 0.94, precision은 약 0.82, recall은 0.9, f1_score는 약 0.86로 모델 성능은 꽤 높은 것으로 판단할 수 있다.\n",
    "\n",
    "이렇게 직접 계산해도 좋지만, 아래와 같이 코드를 입력하면 한눈에 보기 쉽게 만들 수 있으며 더 많은 정보를 얻을 수 있다. 모델 성능으로 평가하는 지표의 결과값은 직접 계산한 값과 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96        40\n",
      "           1       0.82      0.90      0.86        10\n",
      "\n",
      "    accuracy                           0.94        50\n",
      "   macro avg       0.90      0.93      0.91        50\n",
      "weighted avg       0.94      0.94      0.94        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roc curve와 auc의 경우, 모델의 성능을 파악하는데 중요한 지표 중 하나이다. 지표의 값과 그래프를 그리기 위해선 아래와 같은 과정이 필요하다. \n",
    "\n",
    "앞서 만들었던 y_predict는 최종 예측값으로 0과 1로만 그 결과가 도출된다. 반면 아래의 y_pred는 0이 될 확률값과 1이 될 확률값을 도출해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:],new_param_bgd)\n",
    "    y_pred.append([(1-p),p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred[:,1])\n",
    "roc_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de7xVc/7H8denqFBiZBgqRblUKjnTBcldcqkRybXcGnIZuQwzZogxP+NuDIbEuBciyuSupEYlSrpIN+oQKkXopMvn98d3nc7uOGeffS5r733Ofj8fj/1or8te67NX++zP/n6/a32WuTsiIiKlqZXpAEREJLspUYiISFJKFCIikpQShYiIJKVEISIiSSlRiIhIUkoUkjIzO93MXs90HNnEzH4ws90zsN9mZuZmtkW69x0HM5tlZodU4HX6TKaBEkU1ZWafmdma6IvqKzN71Mzqx7lPd3/K3Y+Kcx+JzOwAM3vbzFab2XdmNtrMWqVr/yXEM87Mzkuc5+713X1hTPvb08yeM7Pl0fufYWaXm1ntOPZXUVHCalGZbbh7a3cfV8Z+fpEc0/2ZzFVKFNXb8e5eH2gP7Af8KcPxVEhJv4rNrAvwOvASsAvQHPgImBjHL/hs+2VuZnsAk4ElwL7u3hA4GcgDGlTxvjL23rPtuEsp3F2PavgAPgOOSJi+FfhvwnRd4HZgMfA18ACwVcLynsB04HtgAdA9mt8QeBhYCnwB3ATUjpb1ByZEzx8Abi8W00vA5dHzXYDngWXAIuDShPUGAyOAJ6P9n1fC+3sXuL+E+a8Aj0fPDwHygT8Dy6NjcnoqxyDhtVcDXwFPANsDL0cxr4yeN47W/zuwASgAfgDujeY70CJ6/ihwH/BfYDXhi36PhHiOAuYC3wH3A++U9N6jdZ9M/P8sYXmzaN/9ove3HLg2YXlH4D1gVfR/eS9QJ2G5AxcB84BF0bx/EhLT98AHQNeE9WtHx3lB9N4+AJoA46Nt/Rgdl1Oi9Y8jfL5WAf8D2hb77F4NzADWAluQ8HmOYp8axfE1cGc0f3G0rx+iRxcSPpPROq2BN4Bvo9f+OdN/qzXhkfEA9Kjgf9zmf1iNgY+BfyYsvxsYBfyK8At0NHBztKxj9GV1JKFVuSuwd7TsReBBYBvg18AU4PfRsk1/lMDB0ZeKRdPbA2sICaJW9EVyHVAH2B1YCBwdrTsYWAf0itbdqth725rwpXxoCe/7bGBp9PwQYD1wJyEpdIu+sPZK4RgUvvaW6LVbATsAvaP9NwCeA15M2Pc4in2x88tE8W10fLcAngKGR8saRV98J0bL/hAdg9ISxVfA2Un+/5tF+34oir0d4Ut3n2j5/kDnaF/NgDnAZcXifiM6NoXJ84zoGGwBXBHFUC9adhXhM7YXYNH+dih+DKLpDsA3QCdCgulH+LzWTfjsTickmq0S5hV+nt8Dzoye1wc6F3vPWyTsqz9Fn8kGhKR4BVAvmu6U6b/VmvDIeAB6VPA/Lvxh/UD4defAW8B20TIjfGEm/prtQtEvxweBu0rY5k7Rl01iy+NUYGz0PPGP0gi/8A6Ops8H3o6edwIWF9v2n4D/RM8HA+OTvLfG0Xvau4Rl3YF10fNDCF/22yQsfxb4awrH4BDg58IvwlLiaA+sTJgeR9mJYmjCsh7AJ9Hzs4D3EpYZIdGWlijWEbXySlle+KXZOGHeFKBvKetfBowsFvdhZXzGVgLtoudzgZ6lrFc8Ufwb+FuxdeYC3RI+u+eU8HkuTBTjgRuARqW859ISxanAtDj/7nL1of7B6q2Xu79pZt2Apwm/WlcBOxJ+FX9gZoXrGuHXHYRfcmNK2N5uwJbA0oTX1SJ8oW3G3d3MhhP+OMcDpxG6Swq3s4uZrUp4SW1Cd1KhX2wzwUpgI/Ab4JNiy35D6GbZtK67/5gw/TmhVVPWMQBY5u4FmxaabQ3cRUhG20ezG5hZbXffkCTeRF8lPP+J8IuYKKZN7zk6fvlJtrOC8F4rtD8z25PQ0sojHIctCK28RJv9H5jZFcB5UawObEv4TEH4zCxIIR4I///9zOyShHl1ou2WuO9izgVuBD4xs0XADe7+cgr7LU+MUg4azK4B3P0dwq/Z26NZywndQK3dfbvo0dDDwDeEP9I9StjUEkKLolHC67Z199al7HoYcJKZ7UZoRTyfsJ1FCdvYzt0buHuPxLCTvJ8fCd0PJ5ewuA+h9VRoezPbJmG6KfBlCsegpBiuIHStdHL3bQndaxASTNKYU7CU0FIKGwzZq3Hpq/MmoRusov5NSLIto/fyZ4reR6FN78fMuhLGDfoA27v7doTuycLXlPaZKckS4O/F/v+3dvdhJe27OHef5+6nEro+bwFGRP/HZR3/8sQo5aBEUXPcDRxpZu3dfSOh7/ouM/s1gJntamZHR+s+DJxtZoebWa1o2d7uvpRwptEdZrZttGyPqMXyC+4+jTDwOxR4zd0LWxBTgO/N7Goz28rMaptZGzP7bTnezzWEX6WXmlkDM9vezG4idB/dUGzdG8ysTvRldxzwXArHoCQNCMlllZn9Cri+2PKvCeMtFfFfYF8z6xWd6XMRsHOS9a8HDjCz28xs5yj+Fmb2pJltl8L+GhDGRH4ws72BC1NYfz3h/3MLM7uO0KIoNBT4m5m1tKCtme0QLSt+XB4CLjCzTtG625jZsWaW0tlaZnaGme0Y/R8WfqY2RLFtpPT/g5eBnc3sMjOrG31uOqWyT0lOiaKGcPdlwOOE/nkIvw7nA5PM7HvCL9S9onWnEAaF7yL8anyH0F0AoS+9DjCb0AU0guRdIMOAIwhdX4WxbACOJ/TxLyL8uh9KOKMq1fczATiaMPi7lNCltB9wkLvPS1j1qyjOLwmDxxe4e2F3VanHoBR3EwaGlwOTgFeLLf8noQW10szuSfW9RO9nOaGFdCuhW6kV4cyetaWsv4CQFJsBs8zsO0KLbSphXKosVxK6A1cTvrifKWP91whnlH1KONYFbN49dCdh/Od1QgJ6mHCsIIw5PWZmq8ysj7tPJYxZ3Uv4v5lPGEtIVXfCe/6BcMz7unuBu/9EOPtsYrSvzokvcvfVhBM0jid8LuYBh5Zjv1KKwjNWRKqd6EreJ909WRdOVjKzWoTTc09397GZjkckGbUoRNLEzI42s+3MrC5FYwaTMhyWSJliSxRm9oiZfWNmM0tZbmZ2j5nNj0oTdIgrFpEs0YVwVs5yQvdIL3dfk9mQRMoWW9eTmR1MOM//cXdvU8LyHsAlhHPNOxEuFtPAk4hIlomtReHu4wlXqZamJyGJuLtPArYzs1TOGxcRkTTK5AV3u7L5WRX50bylxVc0swHAAIBtttlm/7333jstAVbU3LmwZg1stVXZ64qIxGmntZ9Tf/0qPvL1y919x4psI5OJovjFP1DKBTXuPgQYApCXl+dTp06NM65KO+SQ8O+4cZmMQkRyVuGQghn8+9/wzTfY4MGfV3RzmTzrKZ9wyX2hxoRz4UVEpKK++AJ69oSno0ubLrwQri9+7Wj5ZDJRjALOis5+6gx8F10ZLCIi5eUODz0ErVrBm2/CDz9U2aZj63oys2GECp2NouJn1xMKzuHuDxCK0vUgXLX5E+FKYRERKa8FC+D882HsWDj00JAw9qi6slexJYqoqFey5YU3ThERkcr4+GP44AMYMgTOOy+MTVQhlRkXEamOZs6EDz+Es86CXr1g4ULYYYeyX1cBKuEhIlKd/PwzDB4MHTrAtddCQXRLlZiSBChRiIhUH5MnhwRxww1wyikwbRrUqxf7btX1JCJSHXzxBXTtCjvtBC+/DMcem7Zdq0UhIpLNPv00/LvrrvDMMzBrVlqTBChRiIhkp1WrYMAA2HtvGD8+zPvd72DbbZO/LgbqehIRyTajRoUrqr/6Cq66Cn5bnrsIVz0lChGRbHLeefDww7DvvvDSS5CXl+mIlChERDIusYhfXh7sthtcfTXUqZPZuCJKFCIimbRkCVxwAfTtC2eeGZ5nGQ1mi4hkwsaNoQR469bhngRr12Y6olKpRSEikm7z5oWxiPHj4YgjQo2m5s0zHVWplChERNJt9myYMQMeeQT696/yIn5VTYlCRCQdPvoIpk+Hfv3CjYUWLoTtt890VCnRGIWISJzWroW//jWczfTXvxYV8asmSQJysEUxZEjRHQLjMn06tG8f7z5EpBp47z0491yYMyeUA7/zzrQU8atqOdeiePrp8EUep/bt4bTT4t2HiGS5L76Abt3CLUnHjIHHHou1FHiccq5FAeGLfNy4TEchIjXSnDmwzz6hiN+zz8Lhh0ODBpmOqlJyrkUhIhKLlSvhnHOgVSt4990wr1evap8kIEdbFCIiVWrkSBg4EJYtgz/9KeNF/KqaEoWISGWccw785z+hT/u//w13oKthlChERMorsYhf587QsiVceSVsuWVm44qJEoWISHl8/jn8/vfh1Mazzgo3F6rhNJgtIpKKjRvhvvugTRuYMAHWrct0RGmjFoWISFnmzg1F/CZMgKOOggcfhGbNMh1V2ihRiIiUZe5cmDULHn00dDdleRG/qqZEISJSkmnTQhmHs8+GE04IRfy22y7TUWWExihERBIVFMCf/xyuhRg8uKiIX44mCVCiEBEpMnFiuB7i5ptDF9P06dWyiF9VU9eTiAiEIn6HHhpqNL32Whi0FkAtChHJdbNnh3933RWefx4+/lhJohglChHJTd9+G25D2rp1uHc1wPHHQ/36GQ0rG6nrSURyz/PPw0UXwYoVcO210LFjpiPKakoUIpJb+vcPNxHq0AFefVW3o0yBEoWI1HyJRfwOOCDcWOiKK2ALfQWmItYxCjPrbmZzzWy+mV1TwvKmZjbWzKaZ2Qwz6xFnPCKSgxYtCoPTjz8epgcMgKuvVpIoh9gShZnVBu4DjgFaAaeaWatiq/0FeNbd9wP6AvfHFY+I5JgNG+Cee0IRv0mTiloVUm5xtig6AvPdfaG7/wwMB3oWW8eBbaPnDYEvY4xHRHLFnDnQtSv84Q/QrVuo09S/f6ajqrbibHvtCixJmM4HOhVbZzDwupldAmwDHFHShsxsADAAoGnTplUeqIjUMPPnh0J+TzwBp5+ec0X8qlqcLYqS/meKt/1OBR5198ZAD+AJM/tFTO4+xN3z3D1vxx13jCFUEan2PvgAHnkkPD/++DA2ccYZShJVIM5EkQ80SZhuzC+7ls4FngVw9/eAekCjGGMSkZpmzRq45hro1An+9reiIn7bbpv8dZKyOBPF+0BLM2tuZnUIg9Wjiq2zGDgcwMz2ISSKZTHGJCI1yfjx0K4d3HJLGIOYNk1F/GIQ2xiFu683s4uB14DawCPuPsvMbgSmuvso4ArgITMbROiW6u+uUxNEJAVffAGHHw5NmsCbb4bnEotYTyR29zHAmGLzrkt4Phs4MM4YRKSG+fhj2HffUMRv5MhQ8XWbbTIdVY2mooAiUj0sXw5nnglt2xYV8TvuOCWJNNCliSKS3dzhuefg4oth5Uq4/vowcC1po0QhItmtX79wPUReHrz1Vuh2krRSohCR7JNYxK9bt9DddNllqs+UIRqjEJHssnAhHHEEPPpomD73XLjySiWJDFKiEJHssGED3H136Fp6/32opa+nbKEULSKZN3s2nHMOTJ4Mxx4LDzwAjRtnOiqJKFGISOYtWgQLFsDTT0PfvqrPlGWUKEQkM95/H6ZPh/PPD62IhQuhQYNMRyUlUCegiKTXTz+FwenOneHmm4uK+ClJZC0lChFJn3Hjwqmud9wRWhIq4lctqOtJRNIjPx+OPBJ22w3efjvUaJJqQS0KEYnXRx+Ffxs3hpdeghkzlCSqGSUKEYnHsmVw2mnQvj28806Y16MHbL11ZuOSclPXk4hULXcYPhwuvRS++w5uuAG6dMl0VFIJKSWK6A51Td19fszxiEh1d+aZ8NRTocLrww9D69aZjkgqqcyuJzM7FvgYeCOabm9mI+MOTESqkY0biwr5HXoo3HknTJyoJFFDpDJGcSPQCVgF4O7TgRZxBiUi1cj8+eE2pP/5T5g+91wYNAhq185sXFJlUkkU69x9VbF5uq+1SK5bvx5uvz0U8Zs2DerUyXREEpNUxijmmFkfoJaZNQf+AEyKNywRyWozZ8LZZ8PUqdCzJ9x/P+yyS6ajkpik0qK4GNgf2Ai8ABQQkoWI5KrFi+Hzz8PZTSNHKknUcKm0KI5296uBqwtnmNmJhKQhIrli8uRw8dyAAeF6iIULoX79TEclaZBKi+IvJcy7tqoDEZEs9eOPcPnl4VqIW2+FtWvDfCWJnFFqi8LMjga6A7ua2Z0Ji7YldEOJSE339tuheN/ChXDhhfCPf0DdupmOStIsWdfTN8BMwpjErIT5q4Fr4gxKRLJAfj4cfTQ0bx5KcBx8cKYjkgwpNVG4+zRgmpk95e4FaYxJRDJp2jTYb79QxG/0aOjWDbbaKtNRSQalMkaxq5kNN7MZZvZp4SP2yEQkvb7+Gk45BTp0KCri1727koSklCgeBf4DGHAM8CwwPMaYRCSd3OHJJ6FVK3jxRbjpJjjggExHJVkklUSxtbu/BuDuC9z9L4CKyYvUFKedFgr57bVXuIf1tdfClltmOirJIqlcR7HWzAxYYGYXAF8Av443LBGJ1caNYBYeRx0VTn296CLVZ5ISpdKiGATUBy4FDgTOB86JMygRidGnn4YKr488EqbPPjvcO0JJQkpRZovC3SdHT1cDZwKYWeM4gxKRGKxfH8p/X3891KunQWpJWdIWhZn91sx6mVmjaLq1mT2OigKKVC8zZkDnznD11XDMMTB7dhibEElBqYnCzG4GngJOB141s2uBscBHwJ7pCU9EqkR+PixZAs89B88/D7/5TaYjkmokWddTT6Cdu68xs18BX0bTc1PduJl1B/4J1AaGuvs/SlinDzCYcI+Lj9xdP3NEqsL//hdaEhdcUFTEb5ttMh2VVEPJup4K3H0NgLt/C3xSziRRG7iPcO1FK+BUM2tVbJ2WwJ+AA929NXBZOeMXkeJ++AH+8Ac46CC4446iIn5KElJByVoUu5tZYSlxA5olTOPuJ5ax7Y7AfHdfCGBmwwmtlNkJ65wP3OfuK6NtflPO+EUk0euvhzLgixeH013/7/9UxE8qLVmi6F1s+t5ybntXYEnCdD7h3tuJ9gQws4mE7qnB7v5q8Q2Z2QBgAEDTpk3LGYZIjliyBI49FvbYA8aPDy0KkSqQrCjgW5XctpW02RL23xI4BGgMvGtmbYrfo9vdhwBDAPLy8nS/bpFEH3wA++8PTZrAmDHQtWs4/VWkiqRywV1F5QNNEqYbEwbEi6/zkruvc/dFwFxC4hCRsnz1FZx8MuTlFRXxO/JIJQmpcnEmiveBlmbW3MzqAH2BUcXWeZGoblR0rcaewMIYYxKp/tzhscdCEb/Ro8M4hIr4SYxSqfUEgJnVdfe1qa7v7uvN7GLgNcL4wyPuPsvMbgSmuvuoaNlRZjYb2ABc5e4ryvcWRHJM377w7LNw4IEwdCjsvXemI5IarsxEYWYdgYeBhkBTM2sHnOful5T1WncfA4wpNu+6hOcOXB49RKQ0iUX8evQI4xADB0KtODsFRIJUPmX3AMcBKwDc/SNUZlwkfT75JNyG9OGHw3S/fnDxxUoSkjapfNJqufvnxeZtiCMYEUmwbl0Yf2jXLtRmql8/0xFJjkpljGJJ1P3k0dXWlwC6FapInKZPD+W/p0+Hk06Cf/0Ldt4501FJjkolUVxI6H5qCnwNvBnNE5G4fPVVeDz/PJxYVhEEkXilkijWu3vf2CMRyXUTJoQifgMHQvfusGABbL11pqMSSWmM4n0zG2Nm/cysQewRieSa1avD4HTXrnD33UVF/JQkJEuUmSjcfQ/gJmB/4GMze9HM1MIQqQqvvQZt2sD994eKrx9+qCJ+knVSOr/O3f/n7pcCHYDvCTc0EpHKWLIEjjsutBwmTAitCZ3ZJFmozERhZvXN7HQzGw1MAZYBqhcgUhHuMGVKeN6kCbzyCkybphIcktVSaVHMBDoDt7p7C3e/wt0nxxyXSM2zdCn07g2dOhUV8TviCBXxk6yXyllPu7v7xtgjEamp3OHRR+Hyy6GgAG65JdRpEqkmSk0UZnaHu18BPG9mv7gHRAp3uBMRgD59YMSIcFbT0KGw556ZjkikXJK1KJ6J/i3vne1EZMOGUMCvVi04/ng47DD4/e9Vn0mqpVI/te4ejbixj7u/lfgA9klPeCLV0Jw5ofVQWMTvrLPgwguVJKTaSuWTe04J886t6kBEqr116+Cmm6B9e5g7Fxo2zHREIlUi2RjFKYS70jU3sxcSFjUAVpX8KpEcNW0a9O8fSnCccgrccw/8+teZjkqkSiQbo5hCuAdFY+C+hPmrgWlxBiVS7Xz9NSxfDi++CD17ZjoakSpVaqJw90XAIkK1WBEpbvx4+PhjuOiiUMRv/nzYaqtMRyVS5UodozCzd6J/V5rZtwmPlWb2bfpCFMky338fKrx26xa6mAqL+ClJSA2VbDC78HanjYAdEx6F0yK5Z8wYaN0aHnwwXECnIn6SA5KdHlt4NXYToLa7bwC6AL8HtklDbCLZZcmSMP7QsCH8739wxx2wjf4UpOZL5fTYFwm3Qd0DeJxwDcXTsUYlki3cYdKk8LxJE3j99dCK6NQps3GJpFEqiWKju68DTgTudvdLgF3jDUskC3z5JfTqBV26FBXxO/RQqFMns3GJpFkqiWK9mZ0MnAm8HM3bMr6QRDLMPdRkatUqtCBuv11F/CSnpVI99hxgIKHM+EIzaw4MizcskQw66SR44YVwVtPQodCiRaYjEsmoMhOFu880s0uBFma2NzDf3f8ef2giaZRYxK9XLzjqKDj/fNVnEiG1O9x1BeYDDwOPAJ+amdrhUnPMnBm6lgqL+J15piq9iiRI5S/hLqCHux/o7gcAxwL/jDcskTT4+We44Qbo0AEWLIDtt890RCJZKZUxijruPrtwwt3nmJlO+5Dq7YMPQhG/mTPhtNPg7rthR11HKlKSVBLFh2b2IPBENH06Kgoo1d2KFbBqFYweDccdl+loRLJaKoniAuBS4I+AAeOBf8UZlEgsxo4NRfwuvTQMVs+bB/XqZToqkayXNFGY2b7AHsBId781PSGJVLHvvoM//hGGDIG99w4D1XXrKkmIpChZ9dg/E8p3nA68YWYl3elOJLuNHh0unBs6FK68MoxNqIifSLkka1GcDrR19x/NbEdgDOH0WJHqYckS6N07tCJefBF++9tMRyRSLSU7PXatu/8I4O7LylhXJDu4h8quUFTEb+pUJQmRSkj25b+7mb0QPUYCeyRMv5DkdZuYWXczm2tm883smiTrnWRmbmZ55X0DIpvk58MJJ4SL5wqL+B1yiIr4iVRSsq6n3sWm7y3Phs2sNuFe20cC+cD7ZjYq8ZqMaL0GhLOqJpdn+yKbbNwIDz0EV10F69fDnXfCQQdlOiqRGiPZPbPfquS2OxLqQi0EMLPhQE9gdrH1/gbcClxZyf1JrurdO4xBHHZYSBi7757piERqlDjHHXYFliRM51PsPhZmth/QxN1fJgkzG2BmU81s6rJly6o+Uql+1q8PLQkIieKhh+DNN5UkRGIQZ6KwEub5poVmtQh1pK4oa0PuPsTd89w9b0eVWZAZM8LNhB56KEyfcQacd16o/ioiVS7lRGFm5T35PJ9wv+1CjYEvE6YbAG2AcWb2GdAZGKUBbSnV2rVw/fWw//7w+eeqzSSSJqmUGe9oZh8D86LpdmaWSgmP94GWZtY8KiLYFxhVuNDdv3P3Ru7ezN2bAZOAE9x9akXeiNRw778fqrzeeCOceirMmQMnnpjpqERyQiotinuA44AVAO7+EXBoWS9y9/XAxcBrwBzgWXefZWY3mtkJFQ9ZctLKlfDDDzBmDDz+OOywQ6YjEskZqRQFrOXun9vm/b8bUtm4u48hXNGdOO+6UtY9JJVtSg55++1QxO8PfwhF/D79VOU3RDIglRbFEjPrCLiZ1Tazy4BPY45LctmqVeE2pIcfDg8+GMYmQElCJENSSRQXApcDTYGvCYPOF8YZlOSwl14KRfweeSRUfFURP5GMK7Pryd2/IQxEi8Rr8WI4+WTYZx8YNQrydAKcSDYoM1GY2UMkXP9QyN0HxBKR5BZ3mDABunaFpk3DRXOdO6s+k0gWSaXr6U3gregxEfg1sDbOoCRHLF4Mxx4LBx9cVMTv4IOVJESyTCpdT88kTpvZE8AbsUUkNd/GjfDAA3D11aFFcc89KuInksVSOT22uObAblUdiOSQE08Mg9ZHHhluT9qsWaYjEpEkUhmjWEnRGEUt4Fug1HtLiJRo/XqoVSs8TjkFevaE/v1Vn0mkGkiaKCxcZdcO+CKatdHdfzGwLZLURx/BOeeEayMuuCCU4BCRaiPpYHaUFEa6+4booSQhqSsogL/8JZzmmp8PO++c6YhEpAJSOetpipl1iD0SqVmmTIH99oO//x1OPz0U8evVK9NRiUgFlNr1ZGZbRIX9DgLON7MFwI+E+0y4uyt5SOm+/x7WrIFXX4Wjj850NCJSCcnGKKYAHQD9DJTUvP46zJoFgwbBEUfA3LkqvyFSAyRLFAbg7gvSFItUVytXwuWXw6OPQuvWMHBgSBBKEiI1QrJEsaOZXV7aQne/M4Z4pLp54QW46CJYtgz+9Ce47jolCJEaJlmiqA3Up+R7X4uEEhx9+0KbNuGGQvvtl+mIRCQGyRLFUne/MW2RSPXgDuPHQ7duoYjf229Dp06w5ZaZjkxEYpLs9Fi1JGRzn38OxxwDhxxSVMTvoIOUJERquGSJ4vC0RSHZbeNGuPfeMFA9YQL861+hLLiI5IRSu57c/dt0BiJZrFcvGD06XA/x4IOwm2pCiuSSilSPlVywbh3Urh2K+J16Kpx0Epx5por4ieSgVEp4SK758EPo2DHcMwJCojjrLCUJkRylRCFF1qwJ10J07AhffQVNmmQ6IhHJAup6kmDSJOjXDz79NJQEv/122H77TEclIllAiUKCH38M4xJvvBHqNImIRJQoctmrr4YifldcAYcfDp98AnXqZDoqEckyGqPIRStWhG6mY46Bxx6Dn38O85UkRKQESuRAf3YAABINSURBVBS5xB1GjIBWreDpp8Pd595/XwlCRJJS11MuWbwYTjsN2rYN945o1y7TEYlINaAWRU3nHgr3Qbiiety4cIaTkoSIpEiJoiZbtAiOOioMVBcW8TvgANhCDUkRSZ0SRU20YQP885/hPhGTJ8O//60ifiJSYfppWRP17An//S/06BHKcOgKaxGpBCWKmiKxiN+ZZ4b6TKedpvpMIlJpsXY9mVl3M5trZvPN7JoSll9uZrPNbIaZvWVmql9dEVOnQl5e6GICOOUUOP10JQkRqRKxJQozqw3cBxwDtAJONbNWxVabBuS5e1tgBHBrXPHUSGvWwNVXh1uRLlum+0SISCzi7HrqCMx394UAZjYc6AnMLlzB3ccmrD8JOKOsjc6dG+7EWVHTp0P79hV/fdZ4771wdfW8eXDeeXDbbbDddpmOSkRqoDgTxa7AkoTpfKBTkvXPBV4paYGZDQAGhOcdKhVU+/ah677aW7Mm3KL0zTfD6a8iIjGJM1GU1EHuJa5odgaQB3Qrabm7DwGGADRokOfjxlVRhNXNmDGhiN9VV8Fhh8GcObDllpmOSkRquDgHs/OBxPMyGwNfFl/JzI4ArgVOcPe1McZTfS1fDmecAcceC089VVTET0lCRNIgzkTxPtDSzJqbWR2gLzAqcQUz2w94kJAkvokxlurJHYYPh332gWefheuvhylTVMRPRNIqtq4nd19vZhcDrwG1gUfcfZaZ3QhMdfdRwG1AfeA5C6dyLnb3E+KKqdpZvDgMWLdrBw8/DPvum+mIRCQHmXuJwwZZq0GDPF+9emqmw4iPO7z1VtFd5iZNgt/+NlxMJyJSQWb2gbvnVeS1qvWUTRYsCGcwHXlkURG/zp2VJEQko5QossGGDXDnnaFr6YMP4MEHVcRPRLKGaj1lg+OPh1degeOOC2U4GjfOdEQiIpsoUWTKzz+H+0LUqgX9+4dCfn37qj6TiGQddT1lwpQpsP/+cP/9YbpPn1DtVUlCRLKQEkU6/fQTXHEFdOkCK1fCHntkOiIRkTKp6yldJkwI10QsXAi//z3ccgs0bJjpqEREyqREkS6FNxYaO7Zy5W9FRNJMiSJOo0eHwn1//CMceijMnh0GsEVEqhGNUcRh2bJQy/yEE2DYsKIifkoSIlINKVFUJXd4+ulQxG/ECLjxRpg8WUX8RKRa00/cqrR4MZx9Nuy3Xyji17p1piMSEak0tSgqa+NGeO218Hy33eDdd2HiRCUJEakxlCgqY968cKe57t1h/Pgwr2NHFfETkRpFiaIi1q+H226Dtm1h+vTQzaQifiJSQ2mMoiKOOy50N/XsGcpw7LJLpiMSyUrr1q0jPz+fgoKCTIeSM+rVq0fjxo3ZsgpvlawbF6Vq7dpwj+patcIZTRs3wsknqz6TSBKLFi2iQYMG7LDDDpj+VmLn7qxYsYLVq1fTvHnzzZbpxkVxmzQJOnSA++4L0yedFAr56YMvklRBQYGSRBqZGTvssEOVt+CUKJL58UcYNAgOOABWr4aWLTMdkUi1oySRXnEcb41RlObdd0MRv0WLYOBAuPlm2HbbTEclIpJ2alGUZv36MCbxzjuhy0lJQqTaGjlyJGbGJ598smneuHHjOO644zZbr3///owYMQIIA/HXXHMNLVu2pE2bNnTs2JFXXnml0rHcfPPNtGjRgr322ovXCq/BKubtt9+mQ4cOtGnThn79+rF+/fpNMTds2JD27dvTvn17brzxxkrHkwolikQvvhhaDhCK+M2aBQcfnNmYRKTShg0bxkEHHcTw4cNTfs1f//pXli5dysyZM5k5cyajR49m9erVlYpj9uzZDB8+nFmzZvHqq68ycOBANmzYsNk6GzdupF+/fgwfPpyZM2ey22678dhjj21a3rVrV6ZPn8706dO57rrrKhVPqtT1BPD113DJJfDcc2HQ+oorQn0mFfETqTKXXRYuO6pK7dvD3XcnX+eHH35g4sSJjB07lhNOOIHBgweXud2ffvqJhx56iEWLFlG3bl0AdtppJ/r06VOpeF966SX69u1L3bp1ad68OS1atGDKlCl06dJl0zorVqygbt267LnnngAceeSR3HzzzZx77rmV2ndl5HaLwh2eeAJatYKXXoK//z2c4aQifiI1xosvvkj37t3Zc889+dWvfsWHH35Y5mvmz59P06ZN2TaFLudBgwZt6gpKfPzjH//4xbpffPEFTZo02TTduHFjvvjii83WadSoEevWrWPq1HAZwIgRI1iyZMmm5e+99x7t2rXjmGOOYdasWWXGVxVy+yfz4sVw3nmQlxeurt5770xHJFJjlfXLPy7Dhg3jsssuA6Bv374MGzaMDh06lHp2UHnPGrrrrrtSXrek69aK78/MGD58OIMGDWLt2rUcddRRbBH1bnTo0IHPP/+c+vXrM2bMGHr16sW8efPKFW9F5F6iKCzid8wxoYjfxImh2qvqM4nUOCtWrODtt99m5syZmBkbNmzAzLj11lvZYYcdWLly5Wbrf/vttzRq1IgWLVqwePFiVq9eTYMGDZLuY9CgQYwdO/YX8/v27cs111yz2bzGjRtv1jrIz89nlxIqO3Tp0oV3330XgNdff51PP/0UYLMWTo8ePRg4cCDLly+nUaNGZRyJSnL3avWoX39/r7C5c927dnUH93HjKr4dEUnJ7NmzM7r/Bx54wAcMGLDZvIMPPtjHjx/vBQUF3qxZs00xfvbZZ960aVNftWqVu7tfddVV3r9/f1+7dq27u3/55Zf+xBNPVCqemTNnetu2bb2goMAXLlzozZs39/Xr1/9iva+//trd3QsKCvywww7zt956y93dly5d6hs3bnR398mTJ3uTJk02TScq6bgDU72C37u5MUaxfj3cckso4vfxx/Cf/+hsJpEcMGzYMH73u99tNq937948/fTT1K1blyeffJKzzz6b9u3bc9JJJzF06FAaNmwIwE033cSOO+5Iq1ataNOmDb169WLHHXesVDytW7emT58+tGrViu7du3PfffdRO+rN6NGjB19++SUAt912G/vssw9t27bl+OOP57DDDgPCeEWbNm1o164dl156KcOHD0/LBY25Uevp6KPh9dfhxBPDNRE77xxPcCKymTlz5rDPPvtkOoycU9Jxr0ytp5o7RlFQEC6Yq10bBgwIj969Mx2ViEi1UzO7niZODCdYFxbx691bSUJEpIJqVqL44Qe49NJwE6GCAlCTVyTjqlv3dnUXx/GuOYninXegTRu49164+GKYOROOPDLTUYnktHr16rFixQolizTx6H4U9erVq9Lt1qwxiq23DlVfDzww05GICOG6gfz8fJYtW5bpUHJG4R3uqlL1PuvphRfgk0/gz38O0xs26MI5EZESZO0d7sysu5nNNbP5ZnZNCcvrmtkz0fLJZtYspQ1/9VW4y1zv3jByJPz8c5ivJCEiUuViSxRmVhu4DzgGaAWcamatiq12LrDS3VsAdwG3lLXdhutWhEHql18OJcH/9z8V8RMRiVGcLYqOwHx3X+juPwPDgZ7F1ukJFBZaHwEcbmVcZrjT2s/DoPVHH8E114RrJUREJDZxDmbvCixJmM4HOpW2jruvN7PvgB2A5YkrmdkAYEA0udYmTJipSq8ANKLYscphOhZFdCyK6FgU2auiL4wzUZTUMig+cp7KOrj7EGAIgJlNreiATE2jY1FEx6KIjkURHYsiZlbO2kdF4ux6ygeaJEw3Br4sbR0z2wJoCHwbY0wiIlJOcSaK94GWZtbczOoAfYFRxdYZBfSLnp8EvO3V7XxdEZEaLraup2jM4WLgNaA28Ii7zzKzGwl10UcBDwNPmNl8QkuibwqbHhJXzNWQjkURHYsiOhZFdCyKVPhYVLsL7kREJL1qTq0nERGJhRKFiIgklbWJIrbyH9VQCsficjObbWYzzOwtM9stE3GmQ1nHImG9k8zMzazGnhqZyrEwsz7RZ2OWmT2d7hjTJYW/kaZmNtbMpkV/Jz0yEWfczOwRM/vGzGaWstzM7J7oOM0wsw4pbbiiN9uO80EY/F4A7A7UAT4CWhVbZyDwQPS8L/BMpuPO4LE4FNg6en5hLh+LaL0GwHhgEpCX6bgz+LloCUwDto+mf53puDN4LIYAF0bPWwGfZTrumI7FwUAHYGYpy3sArxCuYesMTE5lu9naooil/Ec1VeaxcPex7v5TNDmJcM1KTZTK5wLgb8CtQEE6g0uzVI7F+cB97r4SwN2/SXOM6ZLKsXBg2+h5Q355TVeN4O7jSX4tWk/gcQ8mAduZ2W/K2m62JoqSyn/sWto67r4eKCz/UdOkciwSnUv4xVATlXkszGw/oIm7v5zOwDIglc/FnsCeZjbRzCaZWfe0RZdeqRyLwcAZZpYPjAEuSU9oWae83ydA9t64qMrKf9QAKb9PMzsDyAO6xRpR5iQ9FmZWi1CFuH+6AsqgVD4XWxC6nw4htDLfNbM27r4q5tjSLZVjcSrwqLvfYWZdCNdvtXH3jfGHl1Uq9L2ZrS0Klf8oksqxwMyOAK4FTnD3tWmKLd3KOhYNgDbAODP7jNAHO6qGDmin+jfykruvc/dFwFxC4qhpUjkW5wLPArj7e0A9QsHAXJPS90lx2ZooVP6jSJnHIupueZCQJGpqPzSUcSzc/Tt3b+Tuzdy9GWG85gR3r3AxtCyWyt/Ii4QTHTCzRoSuqIVpjTI9UjkWi4HDAcxsH0KiyMX7s44CzorOfuoMfOfuS8t6UVZ2PXl85T+qnRSPxW1AfeC5aDx/sbufkLGgY5LiscgJKR6L14CjzGw2sAG4yt1XZC7qeKR4LK4AHjKzQYSulv418YelmQ0jdDU2isZjrge2BHD3BwjjMz2A+cBPwNkpbbcGHisREalC2dr1JCIiWUKJQkREklKiEBGRpJQoREQkKSUKERFJSolCso6ZbTCz6QmPZknWbVZapcxy7nNcVH30o6jkxV4V2MYFZnZW9Ly/me2SsGyombWq4jjfN7P2KbzmMjPburL7ltylRCHZaI27t094fJam/Z7u7u0IxSZvK++L3f0Bd388muwP7JKw7Dx3n10lURbFeT+pxXkZoEQhFaZEIdVC1HJ418w+jB4HlLBOazObErVCZphZy2j+GQnzHzSz2mXsbjzQInrt4dE9DD6Oav3Xjeb/w4ruAXJ7NG+wmV1pZicRam49Fe1zq6glkGdmF5rZrQkx9zezf1UwzvdIKOhmZv82s6kW7j1xQzTvUkLCGmtmY6N5R5nZe9FxfM7M6pexH8lxShSSjbZK6HYaGc37BjjS3TsApwD3lPC6C4B/unt7whd1flSu4RTgwGj+BuD0MvZ/PPCxmdUDHgVOcfd9CZUMLjSzXwG/A1q7e1vgpsQXu/sIYCrhl397d1+TsHgEcGLC9CnAMxWMszuhTEeha909D2gLdDOztu5+D6GWz6HufmhUyuMvwBHRsZwKXF7GfiTHZWUJD8l5a6Ivy0RbAvdGffIbCHWLinsPuNbMGgMvuPs8Mzsc2B94PypvshUh6ZTkKTNbA3xGKEO9F7DI3T+Nlj8GXATcS7jXxVAz+y+Qcklzd19mZgujOjvzon1MjLZbnji3IZSrSLxDWR8zG0D4u/4N4QY9M4q9tnM0f2K0nzqE4yZSKiUKqS4GAV8D7Qgt4V/clMjdnzazycCxwGtmdh6hrPJj7v6nFPZxemIBQTMr8f4mUW2hjoQic32Bi4HDyvFengH6AJ8AI93dLXxrpxwn4S5u/wDuA040s+bAlcBv3X2lmT1KKHxXnAFvuPup5YhXcpy6nqS6aAgsje4fcCbh1/RmzGx3YGHU3TKK0AXzFnCSmf06WudXlvo9xT8BmplZi2j6TOCdqE+/obuPIQwUl3Tm0WpC2fOSvAD0Itwj4ZloXrnidPd1hC6kzlG31bbAj8B3ZrYTcEwpsUwCDix8T2a2tZmV1DoT2USJQqqL+4F+ZjaJ0O30YwnrnALMNLPpwN6EWz7OJnyhvm5mM4A3CN0yZXL3AkJ1zefM7GNgI/AA4Uv35Wh77xBaO8U9CjxQOJhdbLsrgdnAbu4+JZpX7jijsY87gCvd/SPC/bFnAY8QurMKDQFeMbOx7r6McEbWsGg/kwjHSqRUqh4rIiJJqUUhIiJJKVGIiEhSShQiIpKUEoWIiCSlRCEiIkkpUYiISFJKFCIiktT/A+BsMGOWmbUQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "#https://www.it-swarm.dev/ko/python/python%EC%97%90%EC%84%9C-roc-%EA%B3%A1%EC%84%A0%EC%9D%84-%EA%B7%B8%EB%A6%AC%EB%8A%94-%EB%B0%A9%EB%B2%95/1048726952/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 과정을 통해 만들어진 roc curve는 위와 같고, 이 때 auc 값은 0.95로 나왔다. auc의 경우, 1에 가까울수록 모델의 성능이 좋다는 것을 의미한다. 만들어진 모델은 굉장히 성능이 좋은 것으로 조심스럽게 판단할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
